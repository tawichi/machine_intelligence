Facial Action Coding System

From Wikipedia, the free encyclopedia

The Facial Action Coding System (FACS) is a system to taxonomize human facial movements by their appearance on the face, based on a system originally developed by a Swedish anatomist named Carl-Herman Hjortsj√∂. It was later adopted by Paul Ekman and Wallace V Friesen, and published in 1978. Ekman, Friesen, and Joseph C. Hager published a significant update to FACS in 2002. Movements of individual facial muscles are encoded by the FACS from slight different instant changes in facial appearance. It is a common standard to systematically categorize the physical expression of emotions, and it has proven useful to psychologists and to animators. Due to subjectivity and time consumption issues, the FACS has been established as a computed automated system that detects faces in videos, extracts the geometrical features of the faces, and then produces temporal profiles of each facial movement.

Background

In 2009, a study was conducted to study spontaneous facial expressions in sighted and blind judo athletes. They discovered that many facial expressions are innate and not visually learned.

Uses

Using the FACS human coders can manually code nearly any anatomically possible facial expression, deconstructing it into the specific "action units" (AU) and their temporal segments that produced the expression. As AUs are independent of any interpretation, they can be used for any higher order decision making process including recognition of basic emotions, or pre-programmed commands for an ambient intelligent environment. The FACS manual is over 500 pages in length and provides the AUs, as well as Ekman's interpretation of their meaning.

The FACS defines AUs, as contractions or relaxations of one or more muscles. It also defines a number of "action descriptors" which differ from AUs in that the authors of the FACS have not specified the muscular basis for the action and have not distinguished specific behaviors as precisely as they have for the AUs.

For example, the FACS can be used to distinguish two types of smiles as follows:

- Insincere and voluntary Pan-Am smile: contraction of zygomaticus major alone
- Sincere and involuntary Duchenne smile: contraction of zygomaticus major and inferior part of orbicularis oculi

Although the labeling of expressions currently requires trained experts, researchers have had some success in using computers to automatically identify the FACS codes. Computer graphical face models, such as Artnatomy, allow expressions to be artificially posed by setting the desired action units.

The use of the FACS has been proposed for use in the analysis of depression, and the measurement of pain in patients unable to express themselves verbally.

The FACS is designed to be self-instructional. People can learn the technique from a number of sources including manuals and workshops, and obtain certification through testing. The original FACS has been modified to analyze facial movements in several non-human primates, namely chimpanzees, rhesus macaques, gibbons and siamangs, and orangutans. More recently, it was developed also for domestic species, including dogs, horses and cats. Similarly to the human FACS, the animal FACS has manuals available online for each species with the respective certification tests.

Thus, the FACS can be used to compare facial repertoires across species due to its anatomical basis. A study conducted by Vick and others (2006) suggests that the FACS can be modified by taking differences in underlying morphology into account. Such considerations enable a comparison of the homologous facial movements present in humans and chimpanzees, to show that the facial expressions of both species result from extremely notable appearance changes. The development of FACS tools for different species allows the objective and anatomical study of facial expressions in communicative and emotional contexts. Furthermore, a cross-species analysis of facial expressions can help to answer interesting questions, such as which emotions are uniquely human.

The Emotional Facial Action Coding System (EMFACS) and the Facial Action Coding System Affect Interpretation Dictionary (FACSAID) consider only emotion-related facial actions. Examples of these are:

Emotion    Action units
